4.2.2 卷积神经网络进阶(Vggnet-Resnet)
VGGNet
调参经验

更深层次的神经网络
多使用3*3的卷积核
2个3x3的卷积层可以看做一层5x5的卷积层
3个3x3的卷积层可以看做一层7x7的卷积层
1x1的卷积层可以看做是非线性变换
每经过一个pooling层,通道数目翻倍
视野域：2个3*3 = 1个5*5，因为，他们可以看到的视野区域是一样的

好处：
2层比1层更多一次的非线性变换
参数降低28%（(5*5-2*3*3)/5*5）
多使用1*1的卷积核

1*1的卷积核相当于一次非线性变换。（当卷积核是1*1的时候，相当于是在做全连接，只不过这个全连接是纵向的，是在通道上的）

我们可以通过这种方式对通道进行降维的操作，即使得输出通道的数目小于输入通道的数目

1*1的卷积核可以在加多个这种操作的情况下也不损失信息

从11层递增至19层的过程

每一个版本都是在每一层的后面加一些卷积层，这是因为开始输入的图像比较大，计算比较耗时，不过经过两个maxpooling之后，图像就变的比较小了，这个时候多加几个卷积层之后所耗的损失也没有那么大

LRN 归一化，已经快过时了

基于VGGVet，我们可以配置出各种各样的网络机构，总体参数样本也可以保持不变

训练技巧

先训练浅层网络如A ,再去训练深层网络
多尺度输入
不同的尺度训练多个分类器,然后做ensemble
随机使用不同的尺度缩放然后输入进分类器进行训练
ResNet
VGGNet是将网络层次加深，但是加深到一定程度以后，再加深效果也不能提升效果了。ResNet就是解决了这样一个问题，可以让网络不停的加深，最多可以加深到1000多层

加深层次的问题

模型深度达到某个程度后继续加深会导致训练集准确率下降

加深层次的问题解决

假设：深层网络更难优化而不是学不到东西，ResNet基于这种假设解决了这个加深层次的问题
深层网络至少可以和浅层网络持平
y=x，虽然增加了深度，但误差不会增加
模型结构

Identity部分是恒等变化

这样如果F(x)=0，也就是说F(x)没有学到东西，那么我们可以把他忽略掉，而保留恒等变换X，这样至少可以让他和浅层的神经网络持平

但是他的层次会更深，这样一旦Fx确实学到了一些东西，那么他就可以继续增强这个效果

F(x)是残差学习，ResNet叫做残差网络，这是ResNet的一个基本原理

ResNet-34与ResNet-101使用的子结构（34，101代表层次）

下面是更多的层次结构的说明

方括号里的是残差子结构。经过卷积层之后没有全连接层，因为没有了全连接层，我们就可以将全连接层的参数分摊到卷积层，在这里参数数目可以一定程度上反映模型的容量，参数数目越多，就可以学到更多的东西。

但是参数数目多会导致过拟合，为了使得模型容量是一定的，我们在这里就弱化了全连接层，强化了卷积层

先用一个普通的卷积层, stride=2
再经过一个3*3的max_ _pooling
再经过残差结构
没有中间的全连接层,直接到输出
残差结构使得网络需要学习的知识变少,容易学习

残差结构使得每一层的数据分布接近,容易学习
