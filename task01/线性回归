线性回归的基本要素
模型
为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:

price=warea⋅area+wage⋅age+b
 
数据集
我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。

损失函数
在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为  i  的样本误差的表达式为

l(i)(w,b)=12(y^(i)−y(i))2,
 
L(w,b)=1n∑i=1nl(i)(w,b)=1n∑i=1n12(w⊤x(i)+b−y(i))2.
 
优化函数 - 随机梯度下降
当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。

在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch） B ，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

(w,b)←(w,b)−η|B|∑i∈B∂(w,b)l(i)(w,b)
 
学习率:  η 代表在每次优化中，能够学习的步长的大小
批量大小:  B 是小批量计算中的批量大小batch size
0 什么是回归？
假设线性回归是个黑盒子，那按照程序员的思维来说，这个黑盒子就是个函数，然后呢，我们只要往这个函数传一些参数作为输入，就能得到一个结果作为输出。那回归是什么意思呢？其实说白了，就是这个黑盒子输出的结果是个连续的值。如果输出不是个连续值而是个离散值那就叫分类。那什么叫做连续值呢？非常简单，举个栗子：比如我告诉你我这里有间房子，这间房子有40平，在地铁口，然后你来猜一猜我的房子总共值多少钱？这就是连续值，因为房子可能值80万，也可能值80.2万，也可能值80.111万。再比如，我告诉你我有间房子，120平，在地铁口，总共值180万，然后你来猜猜我这间房子会有几个卧室？那这就是离散值了。因为卧室的个数只可能是1， 2， 3，4，充其量到5个封顶了，而且卧室个数也不可能是什么1.1， 2.9个。所以呢，对于ML萌新来说，你只要知道我要完成的任务是预测一个连续值的话，那这个任务就是回归。是离散值的话就是分类。（PS:目前只讨论监督学习）

1 线性回归
OK，现在既然已经知道什么是回归，那现在就要来聊一聊啥叫线性。其实这玩意也很简单，我们在上初中的时候都学过直线方程对不对？来来来，我们来回忆一下直线方程是啥？
y=kx+b
y=kx+b

喏，这就是初中数学老师教我们的直线方程。那上过初中的同学都知道，这个式子表达的是，当我知道k（参数）和b（参数）的情况下，我随便给一个x我都能通过这个方程算出y来。而且呢，这个式子是线性的，为什么呢？因为从直觉上来说，你都知道，这个式子的函数图像是条直线。。。。从理论上来说，这式子满足线性系统的性质。（至于线性系统是啥，我就不扯了，不然没完没了）那有的同学可能会觉得疑惑，这一节要说的是线性回归，我扯这个low逼直线方程干啥？其实，说白了，线性回归无非就是在N维空间中找一个形式像直线方程一样的函数来拟合数据而已。比如说，我现在有这么一张图，横坐标代表房子的面积，纵坐标代表房价。

然后呢，线性回归就是要找一条直线，并且让这条直线尽可能地拟合图中的数据点。
那如果让1000个老铁来找这条直线就可能找出1000种直线来，比如这样

这样

或者这样

喏，其实找直线的过程就是在做线性回归，只不过这个叫法更有逼格而已。。。
2 损失函数
那既然是找直线，那肯定是要有一个评判的标准，来评判哪条直线才是最好的。OK，道理我们都懂，那咋评判呢？其实简单的雅痞。。。只要算一下实际房价和我找出的直线根据房子大小预测出来的房价之间的差距就行了。说白了就是算两点的距离。当我们把所有实际房价和预测出来的房价的差距（距离）算出来然后做个加和，我们就能量化出现在我们预测的房价和实际房价之间的误差。例如下图中我画了很多条小数线，每一条小数线就是实际房价和预测房价的差距（距离）

然后把每条小竖线的长度加起来就等于我们现在通过这条直线预测出的房价与实际房价之间的差距。那每条小竖线的长度的加和怎么算？其实就是欧式距离加和，公式如下。（其中y(i)表示的是实际房价，y^(i)表示的是预测房价）

这个欧氏距离加和其实就是用来量化预测结果和真实结果的误差的一个函数。在ML中称它为损失函数（说白了就是计算误差的函数）。那有了这个函数，我们就相当于有了一个评判标准，当这个函数的值越小，就越说明我们找到的这条直线越能拟合我们的房价数据。所以说啊，线性回归无非就是通过这个损失函数做为评判标准来找出一条直线。

刚刚我举的例子是一维的例子（特征只有房子大小），那现在我们假设我的数据中还有一个特征是楼间距，那图像可能就是酱紫了。

从图我们可以看得出来，就算是在二维空间中，还是找一条直线来拟合我们的数据。所以啊，换汤不换药，损失函数还是这个欧式距离加和。
