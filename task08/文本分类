前言
作为NLP领域最经典的使用场景之一，文本分类积累了许多的实现方法。这里我们根据是否使用深度学习方法将文本分类主要分为一下两个大类：

基于传统机器学习的文本分类，如 TF-IDF文本分类。
基于深度学习的文本分类，如Facebook开源的FastText文本分类，Text-CNN 文本分类，Text-CNN 文本分类等。
一、基于传统机器学习的文本分类
随着统计学习方法的发展，特别是在90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。整个文本分类问题就拆分成了特征工程和分类器两部分。

1.1 特征工程
这里的特征工程也就是将文本表示为计算机可以识别的、能够代表该文档特征的特征矩阵的过程。在基于传统机器学习的文本分类中，我们通常将特征工程分为文本预处理、特征提取、文本表示等三个部分。

1.1.1 文本预处理
文本预处理过程是提取文本中的关键词来表示文本的过程。中文文本预处理主要包括文本分词和去停用词两个阶段。
文本分词，是因为很多研究表明特征粒度为词粒度远好于字粒度（其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息）。具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统分词算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。
而停止词是文本中一些高频的代词、连词、介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。

1.1.2 特征提取
特征提取包括特征选择和特征权重计算两部分。
特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有：文档频率、互信息、信息增益、χ²统计量等。
特征权重计算主要是经典的TF-IDF方法及其扩展方法。TF-IDF的主要思想是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。

1.1.3 文本表示
文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。

1.2 分类器
大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等等。

二、基于深度学习的文本分类
2.1 FastText文本分类
2.1.1 FastText使用场景
FastText是Facebook AI Research在16年开源的一种文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression等模型，fastText能够在保持分类效果的同时，大大缩短了训练时间。

2.1.2 FastText的几个特点
适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇。
支持多语言表达：利用其语言形态结构，FastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。FastText 的性能要比 Word2Vec 工具明显好上不少。
FastText专注于文本分类。它适合类别特别多的分类问题，如果类别比较少，容易过拟合
2.1.3 FastText原理
FastText方法包含三部分，模型架构，层次SoftMax和N-gram特征。

2.1.3.1 模型架构
FastText模型架构和 Word2Vec 中的 CBOW 模型很类似，因为它们的作者都是Facebook的科学家Tomas Mikolov。不同之处在于，FastText预测标签，而CBOW 模型预测中间词。

CBOW的架构:输入的是w(t)的上下文2d个词，经过隐藏层后，输出的是w(t)。


CBOW模型架构
FastText的架构:将整个文本作为特征去预测文本的类别。


FastText模型架构
2.1.3.2 层次SoftMax
对于有大量类别的数据集，FastText使用了一种分层分类器（而非扁平式架构）。不同的类别被整合进树形结构中（想象下二叉树而非 list）。在某些文本分类任务中类别很多，计算线性分类器的复杂度高。为了改善运行时间，FastText模型使用了层次 Softmax技巧。层次Softmax技巧建立在哈弗曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。
fastText 也利用了类别不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构(Huffman树)。因此，频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高。


Huffman树结构图
2.1.3.3 N-gram特征
FastText 可以用于文本分类和句子分类。不管是文本分类还是句子分类，我们常用的特征是词袋模型。但词袋模型不能考虑词之间的顺序，因此 FastText还加入了 N-gram 特征。“我 爱 她” 这句话中的词袋模型特征是 “我”，“爱”, “她”。这些特征和句子 “她 爱 我” 的特征是一样的。如果加入 2-gram，第一句话的特征还有 “我-爱” 和 “爱-她”，这两句话 “我 爱 她” 和 “她 爱 我” 就能区别开来了。当然啦，为了提高效率，我们需要过滤掉低频的 N-gram。
在 FastText中，每个词被看做是 n-gram字母串包。为了区分前后缀情况，"<"， ">"符号被加到了词的前后端。除了词的子串外，词本身也被包含进了 n-gram字母串包。以 where 为例，n=3 的情况下，其子串分别为<wh, whe, her, ere, re>，以及其本身 。
2.1.4 FastText 和 word2vec 对比
相似点：
模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。
采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。
不同点：
模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用。
模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容。
两者本质的不同，体现在 h-softmax的使用：
Word2vec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的h-softmax也会生成一系列的向量，但最终都被抛弃，不会使用。
fastText则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）。
2.2 Text-CNN文本分类
TextCNN 是利用卷积神经网络对文本进行分类的算法，它是由 Yoon Kim 在2014年在 “Convolutional Neural Networks for Sentence Classification” 一文中提出的。详细的原理图如下。
Text-CNN文本分类模型原理图

TextCNN详细过程：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。
特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。

通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。

一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。

Pooling层：利用CNN解决文本分类问题的文章还是很多的，比如这篇 A Convolutional Neural Network for Modelling Sentences 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。

参考文献

1. FastText算法原理解析
2. FastText原理总结
3. 文本分类解决方法综述
