本文主要讨论在优化模型求解中用到的高级优化算法，这些算法基于常见的优化算法思想，采用一些非常巧妙的技巧，而使得算法到达更快的收敛等特点。这些高级优化算法也只限定于特定的模型求解，下面将对其进行简单总结。

1. Surrogate方法
首先我们考虑一些简单的优化问题minx12∥x−x0∥22+λ∥x∥1minx⁡12‖x−x0‖22+λ‖x‖1，事实上我们只需要对xx向量中的每一个元素进行展开求解即可，它的优化解的表达式如下：
x∗=Sλ(x0)={0∥x0∥2≤λx0−sign(x0)∗λotherwise
x∗=Sλ(x0)={0‖x0‖2≤λx0−sign(x0)∗λotherwise

其中，signsign为符号函数。
下面我们考虑ℓ2ℓ2范数正则化约束，问题形式化为minx12∥x−x0∥22+λ∥x∥2minx⁡12‖x−x0‖22+λ‖x‖2，同理我们只需要对xx向量中的每一个元素进行展开求解即可，它的优化解的表达式如下：
x∗=[(1−λ∥x0∥2)∗x0]+={0∥x0∥2≤λ(1−λ∥x0∥2)∗x0otherwise
x∗=[(1−λ‖x0‖2)∗x0]+={0‖x0‖2≤λ(1−λ‖x0‖2)∗x0otherwise

考虑ℓ∞ℓ∞范数正则化约束，问题形式化为minx12∥x−x0∥22+λ∥x∥∞minx⁡12‖x−x0‖22+λ‖x‖∞。由于ℓ∞ℓ∞的共轭为ℓ1ℓ1barrior函数，因此上述问题的对偶形式为
miny12∥y−x0∥22s.t.∥y∥1<λ
miny⁡12‖y−x0‖22s.t.‖y‖1<λ

其中y=x0−xy=x0−x。通过转换的形式则通过ℓ1ℓ1约束问题很好求解。下面介绍另一种求解ℓ∞ℓ∞范数正则化约束问题。首先判断∥x0∥1‖x0‖1的取值，如果小于等于λλ，则x=0x=0；否则，我们对x0x0向量中每一个元素取绝对值，并安降序排列，记为{v1,⋯,vj,⋯,vM}{v1,⋯,vj,⋯,vM}。取j^=max{j:λ−∑jr=1(vr−vj)>0}j^=max{j:λ−∑r=1j(vr−vj)>0}。则最后xx的优化解形式如下：
x∗i=sign(x0,i)min(vi,(∑r=1j^vr−λ)/j^)i=1,⋯,M
xi∗=sign(x0,i)min(vi,(∑r=1j^vr−λ)/j^)i=1,⋯,M

下面我们继续考虑矩阵ℓ∗ℓ∗核范数正则化约束问题，该问题可以形式化为minA12∥X−A∥2F+∥A∥∗minA⁡12‖X−A‖F2+‖A‖∗，这类问题的求解一般采用矩阵的SVD分解，X=USVTX=USVT；再使用Surrogate策略。一般优化解的形式如下：
A=U∗S^∗VT
A=U∗S^∗VT

其中S^S^的表达式如下：
S^=T(S)=⎧⎩⎨S−ϵS>ϵS+ϵS<−ϵ0otherwise
S^=T(S)={S−ϵS>ϵS+ϵS<−ϵ0otherwise
2. Accelerated Gradient Algorithm
我们考虑一个常见的优化问题，形式化为minWf(W)+λψ(W)minW⁡f(W)+λψ(W)，函数的具体形式视情况而定。比如在机器学习领域，f(W)f(W)一般为总体训练样本的损失函数，f(W)=1N∑nℓ(χn,W)f(W)=1N∑nℓ(χn,W)，χn={xn,yn}χn={xn,yn}为训练样本，而ℓℓ为具体的损失函数，比如平方损失，logistic 损失，hinge损失等；ψ(W)ψ(W)一般为待训练参数WW的正则化约束，这里我们考虑混合约束ψ(W)=∥W∥1,∞ψ(W)=‖W‖1,∞或ψ(W)=∥W∥1,2ψ(W)=‖W‖1,2。该类混合约束在机器学习模型构建中经常用到，比如多任务建模中(f(W)f(W)则为总体任务下总体训练样本的损失，W={wk}kW={wk}k，wkwk为第kk个任务下待训练权重)，所以有必要讨论其优化解。
一般而言，上式问题的求解可以采用子梯度下降法优化WW，但遗憾的是收敛速度较慢。因此在优化目标问题中，对f(W)f(W)在W=WtW=Wt处进行二阶泰勒近似展开，优化目标函数为：
minWf(Wt)+<W−Wt,∇f(Wt)>+L2∥W−Wt∥2F+λψ(W)
minW⁡f(Wt)+<W−Wt,∇f(Wt)>+L2‖W−Wt‖F2+λψ(W)

其中<A,B>=Tr(ATB)<A,B>=Tr(ATB)。下面对上式进行重新整理得：
minW12∥W−(Wt−1L∇f(Wt))∥2F+λL∥W∥1,∞
minW⁡12‖W−(Wt−1L∇f(Wt))‖F2+λL‖W‖1,∞

其中∥W∥1,∞=∑j∥Wj∥∞‖W‖1,∞=∑j‖Wj‖∞，即为每一行元素绝对值的最大值累加。为了简化上式，我们令V=Wt−1L∇f(Wt)V=Wt−1L∇f(Wt)和λ^=λLλ^=λL，则
minW12∥W−V∥2F+λ^∥W∥1,∞
minW⁡12‖W−V‖F2+λ^‖W‖1,∞

同样，上式问题可以各个维度上的子问题求解，下面我们仅考虑矩阵W,VW,V的第ii行，记着w,vw,v。那么子问题简化为：
minw12∥w−v∥22+λ^∥w∥∞
minw⁡12‖w−v‖22+λ^‖w‖∞

如果原始问题为ψ(W)=∥W∥1,2ψ(W)=‖W‖1,2混合范数约束，则此时的优化子问题为：
minw12∥w−v∥22+λ^∥w∥2
minw⁡12‖w−v‖22+λ^‖w‖2

因此上述问题求解。可知，该方法对优化目标函数采用泰勒近似展开达到对原问题的简化，从而加快算法的收敛速度。
